// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.openai.models;

import io.clientcore.core.annotation.Metadata;
import io.clientcore.core.annotation.TypeConditions;
import java.util.List;

/**
 * The CreateTranscriptionRequest model.
 */
@Metadata(conditions = { TypeConditions.FLUENT })
public final class CreateTranscriptionRequest {
    /*
     * The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a,
     * ogg, wav, or webm.
     */
    @Metadata(generated = true)
    private final FileDetails file;

    /*
     * ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently
     * available.
     */
    @Metadata(generated = true)
    private final CreateTranscriptionRequestModel model;

    /*
     * The language of the input audio. Supplying the input language in
     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
     */
    @Metadata(generated = true)
    private String language;

    /*
     * An optional text to guide the model's style or continue a previous audio segment. The
     * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     */
    @Metadata(generated = true)
    private String prompt;

    /*
     * The response_format property.
     */
    @Metadata(generated = true)
    private TranscriptionAudioResponseFormat responseFormat;

    /*
     * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower
     * values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log
     * probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until
     * certain thresholds are hit.
     */
    @Metadata(generated = true)
    private Double temperature;

    /*
     * The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to
     * use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is
     * no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
     */
    @Metadata(generated = true)
    private List<CreateTranscriptionRequestTimestampGranularities> timestampGranularities;

    /**
     * Creates an instance of CreateTranscriptionRequest class.
     * 
     * @param file the file value to set.
     * @param model the model value to set.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest(FileDetails file, CreateTranscriptionRequestModel model) {
        this.file = file;
        this.model = model;
    }

    /**
     * Get the file property: The audio file object (not file name) to transcribe, in one of these formats: flac, mp3,
     * mp4, mpeg, mpga, m4a, ogg, wav, or webm.
     * 
     * @return the file value.
     */
    @Metadata(generated = true)
    public FileDetails getFile() {
        return this.file;
    }

    /**
     * Get the model property: ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2
     * model) is currently available.
     * 
     * @return the model value.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequestModel getModel() {
        return this.model;
    }

    /**
     * Get the language property: The language of the input audio. Supplying the input language in
     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
     * 
     * @return the language value.
     */
    @Metadata(generated = true)
    public String getLanguage() {
        return this.language;
    }

    /**
     * Set the language property: The language of the input audio. Supplying the input language in
     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
     * 
     * @param language the language value to set.
     * @return the CreateTranscriptionRequest object itself.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest setLanguage(String language) {
        this.language = language;
        return this;
    }

    /**
     * Get the prompt property: An optional text to guide the model's style or continue a previous audio segment. The
     * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     * 
     * @return the prompt value.
     */
    @Metadata(generated = true)
    public String getPrompt() {
        return this.prompt;
    }

    /**
     * Set the prompt property: An optional text to guide the model's style or continue a previous audio segment. The
     * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     * 
     * @param prompt the prompt value to set.
     * @return the CreateTranscriptionRequest object itself.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest setPrompt(String prompt) {
        this.prompt = prompt;
        return this;
    }

    /**
     * Get the responseFormat property: The response_format property.
     * 
     * @return the responseFormat value.
     */
    @Metadata(generated = true)
    public TranscriptionAudioResponseFormat getResponseFormat() {
        return this.responseFormat;
    }

    /**
     * Set the responseFormat property: The response_format property.
     * 
     * @param responseFormat the responseFormat value to set.
     * @return the CreateTranscriptionRequest object itself.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest setResponseFormat(TranscriptionAudioResponseFormat responseFormat) {
        this.responseFormat = responseFormat;
        return this;
    }

    /**
     * Get the temperature property: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
     * output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the
     * model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the
     * temperature until certain thresholds are hit.
     * 
     * @return the temperature value.
     */
    @Metadata(generated = true)
    public Double getTemperature() {
        return this.temperature;
    }

    /**
     * Set the temperature property: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the
     * output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the
     * model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the
     * temperature until certain thresholds are hit.
     * 
     * @param temperature the temperature value to set.
     * @return the CreateTranscriptionRequest object itself.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest setTemperature(Double temperature) {
        this.temperature = temperature;
        return this;
    }

    /**
     * Get the timestampGranularities property: The timestamp granularities to populate for this transcription.
     * `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are
     * supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word
     * timestamps incurs additional latency.
     * 
     * @return the timestampGranularities value.
     */
    @Metadata(generated = true)
    public List<CreateTranscriptionRequestTimestampGranularities> getTimestampGranularities() {
        return this.timestampGranularities;
    }

    /**
     * Set the timestampGranularities property: The timestamp granularities to populate for this transcription.
     * `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are
     * supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word
     * timestamps incurs additional latency.
     * 
     * @param timestampGranularities the timestampGranularities value to set.
     * @return the CreateTranscriptionRequest object itself.
     */
    @Metadata(generated = true)
    public CreateTranscriptionRequest
        setTimestampGranularities(List<CreateTranscriptionRequestTimestampGranularities> timestampGranularities) {
        this.timestampGranularities = timestampGranularities;
        return this;
    }
}
